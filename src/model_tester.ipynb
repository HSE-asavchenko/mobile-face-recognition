{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fb98b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "import math\n",
    "import datetime, time\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from scipy.misc import imread\n",
    "from matplotlib.pyplot import imread\n",
    "import cv2\n",
    "#from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PIL import Image\n",
    "\n",
    "import lfw\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137366f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e07fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='D:/datasets/lfw_ytf/'\n",
    "DATASET_PATH=DATA_PATH+'lfw_mtcnn_aligned'\n",
    "#DATASET_PATH=DATA_PATH+'lfw_mtcnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809ed0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_extensions=['.jpg','.jpeg','.png']\n",
    "def is_image(path):\n",
    "    _, file_extension = os.path.splitext(path)\n",
    "    return file_extension.lower() in img_extensions\n",
    "\n",
    "def get_files(db_dir):\n",
    "    return [[d,os.path.join(d,f)] for d in next(os.walk(db_dir))[1] for f in sorted(next(os.walk(os.path.join(db_dir,d)))[2]) if not f.startswith(\".\") and is_image(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f16d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_per_class_cv(y, n_splits=10,random_state=0):\n",
    "    res_cv=[]\n",
    "    inds = np.arange(len(y))\n",
    "    np.random.seed(random_state)\n",
    "    for _ in range(n_splits):\n",
    "        inds_train, inds_test = [], []\n",
    "\n",
    "        for lbl in np.unique(y):\n",
    "            tmp_inds = inds[y == lbl]\n",
    "            np.random.shuffle(tmp_inds)\n",
    "            last_ind=1\n",
    "            #last_ind=math.ceil(len(tmp_inds)/2)\n",
    "            if last_ind==0 and len(tmp_inds)>0:\n",
    "                last_ind=1\n",
    "            inds_train.extend(tmp_inds[:last_ind])\n",
    "            inds_test.extend(tmp_inds[last_ind:])\n",
    "            \n",
    "        inds_train = np.array(inds_train)\n",
    "        inds_test = np.array(inds_test)\n",
    "    \n",
    "        res_cv.append((inds_train, inds_test))\n",
    "    return res_cv\n",
    "\n",
    "def classifier_tester(classifier,x,y):\n",
    "    sss=get_single_image_per_class_cv(y)\n",
    "    #sss=model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "    scores=model_selection.cross_validate(classifier,x, y, scoring='accuracy',cv=sss)\n",
    "    acc=scores['test_score']\n",
    "    print('accuracies=',acc*100)\n",
    "    print('total acc=',round(acc.mean()*100,2),round(acc.std()*100,2))\n",
    "    print('test time=',scores['score_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420d762",
   "metadata": {},
   "source": [
    "## Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "723ca166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after loading: num_classes= 596\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    dirs_and_files=np.array(get_files(DATASET_PATH))\n",
    "else: #LFW and YTF concatenation\n",
    "    subjects = (line.rstrip('\\n') for line in open('lfw_ytf_classes.txt'))\n",
    "    dirs_and_files=np.array([[d,os.path.join(d,f)] for d in subjects for f in sorted(next(os.walk(os.path.join(DATASET_PATH,d)))[2]) if is_image(f)])\n",
    "\n",
    "dirs=dirs_and_files[:,0]\n",
    "files=dirs_and_files[:,1]\n",
    "\n",
    "label_enc=preprocessing.LabelEncoder()\n",
    "label_enc.fit(dirs)\n",
    "y=label_enc.transform(dirs)\n",
    "\n",
    "y_l=list(y)\n",
    "indices=[i for i,el in enumerate(y_l) if y_l.count(el) > 1]\n",
    "y=y[indices]\n",
    "label_enc=preprocessing.LabelEncoder()\n",
    "label_enc.fit(y)\n",
    "y=label_enc.transform(y)\n",
    "print('after loading: num_classes=',len(label_enc.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8bc0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components=128 #256\n",
    "classifiers=[]\n",
    "classifiers.append(['k-NN+PCA',Pipeline(steps=[('pca', PCA(n_components=pca_components)), ('classifier', KNeighborsClassifier(n_neighbors=1,p=2))])])\n",
    "classifiers.append(['k-NN',KNeighborsClassifier(n_neighbors=1,p=2)])\n",
    "classifiers.append(['linear svm',LinearSVC()])\n",
    "\n",
    "def test_faceID():\n",
    "    for cls_name,classifier in classifiers:\n",
    "        print(cls_name)\n",
    "        classifier_tester(classifier,X_norm,y)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f96133",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d612e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\src_code\\mobile\\mobile-face-recognition\\src\\lfw.py:193: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(pairs)\n"
     ]
    }
   ],
   "source": [
    "pairs = lfw.read_pairs(DATA_PATH+'pairs.txt')\n",
    "#print(pairs.shape,pairs[:5],pairs[-5:])\n",
    "# Get the paths for the corresponding images\n",
    "paths, actual_issame = lfw.get_paths(DATASET_PATH, pairs)\n",
    "#print(len(paths), len(actual_issame), paths[:5], actual_issame[:5], paths[-5:], actual_issame[-5:])\n",
    "\n",
    "embedding_indices=np.zeros(len(paths), dtype=int)\n",
    "filepaths={}\n",
    "for i,p in enumerate(paths):\n",
    "    if p not in filepaths:\n",
    "        filepaths[p]=len(filepaths)\n",
    "    embedding_indices[i]=filepaths[p]\n",
    "#print(len(filepaths),len(embedding_indices), len(actual_issame), embedding_indices[:5], actual_issame[:5], embedding_indices[-5:], actual_issame[-5:])\n",
    "\n",
    "files4verification=[None]*len(filepaths)\n",
    "for f,i in filepaths.items():\n",
    "    files4verification[i]=f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e763a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_verification_results(X):\n",
    "    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n",
    "    embedding_size=X.shape[1]\n",
    "    embeddings = np.zeros((nrof_embeddings, embedding_size))\n",
    "    for i,emb_ind in enumerate(embedding_indices):\n",
    "        embeddings[i,:]=X[emb_ind]\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0)\n",
    "\n",
    "    print('Accuracy: %2.5f+-%2.5f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print('Area Under Curve (AUC): %1.5f' % auc)\n",
    "    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    print('Equal Error Rate (EER): %1.3f' % eer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c2c74",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5bcd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c7792",
   "metadata": {},
   "source": [
    "## Our models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48968ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sys.path.append(\"../once-for-all\")\n",
    "from ofa.imagenet_classification.networks.mobilenet_v3 import MobileNetV3\n",
    "from ofa.imagenet_classification.elastic_nn.networks import OFAMobileNetV3\n",
    "\n",
    "def get_model_artifacts(net_folder,model_name):\n",
    "    config_filepath = os.path.join(net_folder, model_name,f\"{model_name}.config\")\n",
    "    test_pt_filepath = os.path.join(net_folder, model_name,f\"{model_name}.pth\")\n",
    "    model_config = json.load(open(config_filepath, 'r'))\n",
    "    test_checkpoint = torch.load(test_pt_filepath, map_location=torch.device('cpu'))\n",
    "    filter_end_fn = lambda x : not x.endswith('total_ops') and not x.endswith('total_params')\n",
    "    filter_start_fn = lambda x : not x.startswith('total_ops') and not x.startswith('total_params')\n",
    "    filtered_state_dict = {key:value for key,value in test_checkpoint['state_dict'].items() if filter_start_fn(key) and filter_end_fn(key)}\n",
    "    model=MobileNetV3.build_from_config(model_config)\n",
    "    model.load_state_dict(filtered_state_dict)\n",
    "    #torch.save(model,model_name+'.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a984451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model(model_name):\n",
    "    print(model_name)\n",
    "    if model_name=='SuperNet':\n",
    "        cnn_model = OFAMobileNetV3(n_classes=9131,dropout_rate=0.,width_mult=1.2, ks_list= [3, 5, 7],expand_ratio_list=[3, 4, 6],depth_list=[2, 3, 4])\n",
    "        state_dict = torch.load('../models/ofa_mbv3_model_best_new.pth.tar', map_location=\"cpu\")[\"state_dict\"]\n",
    "        cnn_model.load_state_dict(state_dict)\n",
    "        #print(cnn_model)\n",
    "    else:\n",
    "        cnn_model=get_model_artifacts('../models/ofa_subnets',model_name)\n",
    "        #cnn_model=get_model_artifacts('D:/src_code/DNN_models/ofa_subnets',model_name)\n",
    "    cnn_model.classifier=torch.nn.Identity()\n",
    "    cnn_model=cnn_model.to(device)\n",
    "    cnn_model.eval()\n",
    "    return cnn_model\n",
    "\n",
    "model_names=[\n",
    "    'SuperNet',\n",
    "\n",
    "    'subnet_device_865_acc_98.3_lut_12.1ms_w12_d234_nac_gbdt',\n",
    "    'subnet_device_865_acc_97.8_lut_9.15ms_w12_d234_nac_gbdt',\n",
    "    'subnet_device_765_acc_98.3_lut_34.6ms_w12_d234_nac_gbdt',\n",
    "    'subnet_device_765_acc_97.7_lut_24.45ms_w12_d234_nac_gbdt',\n",
    "\n",
    "    #'subnet_device_865_acc_97.9_lut_12.1ms_w12_d234_reg_predictor',\n",
    "    #'subnet_device_865_acc_97.7_lut_9.15ms_w12_d234_reg_predictor',\n",
    "    #'subnet_device_765_acc_97.9_lut_34.6ms_w12_d234_reg_predictor',\n",
    "    #'subnet_device_765_acc_97.7_lut_24.45ms_w12_d234_reg_predictor',\n",
    "\n",
    "    #'subnet_device_865_acc_98.3_lut_12.1ms_w12_d234_reg_gbdt',\n",
    "    #'subnet_device_865_acc_97.8_lut_9.15ms_w12_d234_reg_gbdt',\n",
    "    #'subnet_device_765_acc_98.4_lut_34.6ms_w12_d234_reg_gbdt',\n",
    "    #'subnet_device_765_acc_97.7_lut_24.45ms_w12_d234_reg_gbdt',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3044c",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a3d8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_torch_features(model, files,crop_center=False, standard_preprocessing=True):\n",
    "    if standard_preprocessing:\n",
    "        from torchvision import transforms\n",
    "        test_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224,224)),\n",
    "                #transforms.Resize((260,260)),\n",
    "                #transforms.Resize((160,160)),\n",
    "                #transforms.Resize((196,196)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        mean_bgr = np.array([91.4953, 103.8827, 131.0912])\n",
    "        def test_transforms(img):\n",
    "            img = np.array(img.resize((224,224),Image.BILINEAR))\n",
    "            img = img[:, :, ::-1]  # RGB -> BGR\n",
    "            img = img.astype(np.float32)\n",
    "            img -= mean_bgr\n",
    "            img = img.transpose(2, 0, 1)  # C x H x W\n",
    "            img = torch.from_numpy(img).float()\n",
    "            return img\n",
    "    X=[]\n",
    "    imgs=[]\n",
    "    for filepath in files:\n",
    "        img = Image.open(os.path.join(DATASET_PATH,filepath))\n",
    "        if crop_center:\n",
    "            orig_w,orig_h=250,250\n",
    "            img = img.resize((orig_w,orig_h),Image.BILINEAR)\n",
    "            w1,h1=224,224#128,128\n",
    "            dw=(orig_w-w1)/2\n",
    "            dh=(orig_h-h1)/2\n",
    "            box = (dw, dh, orig_w-dw, orig_h-dh)\n",
    "            img = img.crop(box)\n",
    "        #img = img.resize((224,224),Image.BILINEAR)\n",
    "        img_tensor = test_transforms(img)\n",
    "            \n",
    "        if img.size:\n",
    "            imgs.append(img_tensor)\n",
    "            if len(imgs)>=8: #1:#8:\n",
    "                scores = model(torch.stack(imgs, dim=0).to(device))\n",
    "                scores=scores.data.cpu().numpy()\n",
    "                #print(scores.shape)\n",
    "                if len(X)==0:\n",
    "                    X=scores\n",
    "                else:\n",
    "                    X=np.concatenate((X,scores),axis=0)\n",
    "                \n",
    "                imgs=[]\n",
    "\n",
    "    if len(imgs)>0:        \n",
    "        scores = model(torch.stack(imgs, dim=0).to(device))\n",
    "        scores=scores.data.cpu().numpy()\n",
    "        #print(scores.shape)\n",
    "        if len(X)==0:\n",
    "            X=scores\n",
    "        else:\n",
    "            X=np.concatenate((X,scores),axis=0)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aae97dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperNet\n",
      "--- 31.18065595626831 seconds ---\n",
      "after loading: X shape: (3739, 1536)  X_norm shape: (3739, 1536)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [99.1091314  99.36366529 96.72287623 99.1091314  98.98186446 99.0136812\n",
      " 99.0136812  99.20458161 98.72733058 99.33184855]\n",
      "total acc= 98.86 0.73\n",
      "test time= [0.0977385  0.10076785 0.10571551 0.09678173 0.09877229 0.09574366\n",
      " 0.0997715  0.09478331 0.11269784 0.09478498]\n",
      "\n",
      "k-NN\n",
      "accuracies= [99.42729876 99.42729876 96.02290805 99.17276487 99.36366529 99.14094814\n",
      " 99.14094814 99.36366529 99.07731467 99.58638244]\n",
      "total acc= 98.97 1.0\n",
      "test time= [0.15256619 0.16855097 0.16256428 0.15557933 0.16060805 0.14960766\n",
      " 0.15554214 0.17253923 0.1496408  0.15753984]\n",
      "\n",
      "linear svm\n",
      "accuracies= [99.36366529 99.45911549 97.58192809 99.33184855 99.33184855 99.30003182\n",
      " 99.23639835 99.33184855 99.23639835 99.42729876]\n",
      "total acc= 99.16 0.53\n",
      "test time= [0.05688596 0.05487347 0.05385447 0.05387783 0.05484939 0.05385566\n",
      " 0.05386043 0.05389476 0.06085896 0.07177281]\n",
      "\n",
      "subnet_device_865_acc_98.3_lut_12.1ms_w12_d234_nac_gbdt\n",
      "--- 15.058749198913574 seconds ---\n",
      "after loading: X shape: (3739, 1536)  X_norm shape: (3739, 1536)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [98.44097996 99.1091314  91.85491569 98.50461343 98.82278078 98.31371301\n",
      " 98.63188037 98.79096405 98.15462934 98.85459752]\n",
      "total acc= 97.95 2.05\n",
      "test time= [0.1047194  0.09777403 0.09678125 0.10874391 0.0957799  0.09973478\n",
      " 0.09478474 0.09478736 0.10276747 0.10475445]\n",
      "\n",
      "k-NN\n",
      "accuracies= [99.23639835 99.33184855 90.51861279 98.91823099 98.98186446 98.44097996\n",
      " 99.0136812  99.0136812  98.72733058 99.07731467]\n",
      "total acc= 98.13 2.55\n",
      "test time= [0.15454912 0.15458703 0.15263581 0.16655254 0.15255737 0.15562224\n",
      " 0.15358758 0.15355229 0.15163207 0.15159035]\n",
      "\n",
      "linear svm\n",
      "accuracies= [98.95004773 99.23639835 94.9093223  98.88641425 99.07731467 98.53643016\n",
      " 98.91823099 98.75914731 98.69551384 99.04549793]\n",
      "total acc= 98.5 1.21\n",
      "test time= [0.05888247 0.05487514 0.05884147 0.06482673 0.07682443 0.05485487\n",
      " 0.05389667 0.0608027  0.05485773 0.05489182]\n",
      "\n",
      "subnet_device_865_acc_97.8_lut_9.15ms_w12_d234_nac_gbdt\n",
      "--- 12.835652828216553 seconds ---\n",
      "after loading: X shape: (3739, 1536)  X_norm shape: (3739, 1536)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [96.75469297 98.21826281 86.38243716 97.00922685 97.32739421 97.64556157\n",
      " 97.42284442 97.64556157 97.42284442 97.48647789]\n",
      "total acc= 96.33 3.34\n",
      "test time= [0.09976792 0.0997324  0.09678149 0.1027627  0.1007297  0.09474587\n",
      " 0.09973097 0.09378672 0.09574866 0.09574294]\n",
      "\n",
      "k-NN\n",
      "accuracies= [97.80464524 98.37734648 86.25517022 97.58192809 98.31371301 97.83646198\n",
      " 98.34552975 98.25007954 98.1228126  97.99554566]\n",
      "total acc= 96.89 3.55\n",
      "test time= [0.16357565 0.17955399 0.15255284 0.16156721 0.1575799  0.16555858\n",
      " 0.16755128 0.16858768 0.16555643 0.15358686]\n",
      "\n",
      "linear svm\n",
      "accuracies= [97.77282851 98.34552975 91.31403118 97.61374483 98.34552975 97.80464524\n",
      " 98.1228126  97.96372892 97.90009545 98.02736239]\n",
      "total acc= 97.32 2.01\n",
      "test time= [0.06906605 0.06185484 0.05481887 0.05388284 0.07576084 0.06283092\n",
      " 0.05987072 0.06282473 0.05889893 0.07104826]\n",
      "\n",
      "subnet_device_765_acc_98.3_lut_34.6ms_w12_d234_nac_gbdt\n",
      "--- 15.251015424728394 seconds ---\n",
      "after loading: X shape: (3739, 1536)  X_norm shape: (3739, 1536)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [98.5682469  98.85459752 94.43207127 98.5682469  98.95004773 98.44097996\n",
      " 98.63188037 98.88641425 98.02736239 98.63188037]\n",
      "total acc= 98.2 1.28\n",
      "test time= [0.10072994 0.10675693 0.09977221 0.10674667 0.09973192 0.10279965\n",
      " 0.09877348 0.10052872 0.103724   0.09873629]\n",
      "\n",
      "k-NN\n",
      "accuracies= [99.23639835 99.20458161 92.5867006  98.82278078 99.17276487 98.6636971\n",
      " 99.0136812  99.04549793 98.60006363 99.0136812 ]\n",
      "total acc= 98.34 1.93\n",
      "test time= [0.16854692 0.17050886 0.16957974 0.16665053 0.17857099 0.1645205\n",
      " 0.16906214 0.16605544 0.16908956 0.16955638]\n",
      "\n",
      "linear svm\n",
      "accuracies= [98.98186446 99.20458161 95.60929049 99.07731467 99.1091314  98.69551384\n",
      " 98.95004773 98.91823099 98.88641425 99.20458161]\n",
      "total acc= 98.66 1.03\n",
      "test time= [0.061836   0.06127286 0.05984235 0.0619297  0.07180929 0.06188178\n",
      " 0.06087613 0.06381941 0.06095982 0.06195498]\n",
      "\n",
      "subnet_device_765_acc_97.7_lut_24.45ms_w12_d234_nac_gbdt\n",
      "--- 13.21359920501709 seconds ---\n",
      "after loading: X shape: (3739, 1536)  X_norm shape: (3739, 1536)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [96.56379255 97.90009545 85.17340121 96.85014318 97.16831053 97.29557747\n",
      " 97.04104359 97.35921094 97.231944   97.231944  ]\n",
      "total acc= 95.98 3.62\n",
      "test time= [0.10472059 0.10035968 0.10376644 0.1067555  0.10471916 0.10331345\n",
      " 0.10072899 0.09750247 0.10376048 0.10575747]\n",
      "\n",
      "k-NN\n",
      "accuracies= [97.42284442 98.21826281 85.36430162 97.61374483 98.1228126  97.80464524\n",
      " 97.96372892 98.18644607 97.99554566 97.70919504]\n",
      "total acc= 96.64 3.77\n",
      "test time= [0.16252565 0.16359878 0.16751719 0.17450929 0.16805601 0.16606498\n",
      " 0.17005539 0.17556977 0.1744976  0.17756104]\n",
      "\n",
      "linear svm\n",
      "accuracies= [97.55011136 98.25007954 89.46866052 97.70919504 98.18644607 97.74101177\n",
      " 97.86827871 97.6773783  97.83646198 97.93191219]\n",
      "total acc= 97.02 2.53\n",
      "test time= [0.07383704 0.06043959 0.0549047  0.05888796 0.06003928 0.06286883\n",
      " 0.05598092 0.05489206 0.0668571  0.05992341]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    cnn_model=get_my_model(model_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X=extract_torch_features(cnn_model,files)\n",
    "    print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "    X_norm=preprocessing.normalize(X,norm='l2')\n",
    "    X_norm=X_norm[indices,:]\n",
    "    print('after loading: X shape:',X.shape,' X_norm shape:',X_norm.shape)\n",
    "    \n",
    "    print('Recognition:')\n",
    "    test_faceID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "939a2a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification\n",
      "SuperNet\n",
      "--- 67.26128721237183 seconds ---\n",
      "(7701, 1536)\n",
      "Accuracy: 0.99317+-0.00337\n",
      "Validation rate: 0.98000+-0.01366 @ FAR=0.00133\n",
      "Area Under Curve (AUC): 0.99978\n",
      "Equal Error Rate (EER): 0.006\n",
      "subnet_device_865_acc_98.3_lut_12.1ms_w12_d234_nac_gbdt\n",
      "--- 34.714967012405396 seconds ---\n",
      "(7701, 1536)\n",
      "Accuracy: 0.99217+-0.00350\n",
      "Validation rate: 0.98133+-0.01194 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99970\n",
      "Equal Error Rate (EER): 0.008\n",
      "subnet_device_865_acc_97.8_lut_9.15ms_w12_d234_nac_gbdt\n",
      "--- 30.85317611694336 seconds ---\n",
      "(7701, 1536)\n",
      "Accuracy: 0.99200+-0.00414\n",
      "Validation rate: 0.97700+-0.01286 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99955\n",
      "Equal Error Rate (EER): 0.010\n",
      "subnet_device_765_acc_98.3_lut_34.6ms_w12_d234_nac_gbdt\n",
      "--- 35.97784209251404 seconds ---\n",
      "(7701, 1536)\n",
      "Accuracy: 0.99300+-0.00400\n",
      "Validation rate: 0.98133+-0.01352 @ FAR=0.00133\n",
      "Area Under Curve (AUC): 0.99974\n",
      "Equal Error Rate (EER): 0.007\n",
      "subnet_device_765_acc_97.7_lut_24.45ms_w12_d234_nac_gbdt\n",
      "--- 30.631898641586304 seconds ---\n",
      "(7701, 1536)\n",
      "Accuracy: 0.99067+-0.00461\n",
      "Validation rate: 0.97067+-0.01756 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99953\n",
      "Equal Error Rate (EER): 0.010\n"
     ]
    }
   ],
   "source": [
    "print('Verification')\n",
    "for model_name in model_names:\n",
    "    cnn_model=get_my_model(model_name)\n",
    "    start_time = time.time()\n",
    "    X=extract_torch_features(cnn_model,files4verification)\n",
    "    print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "    print(X.shape)\n",
    "    X=preprocessing.normalize(X,norm='l2')\n",
    "\n",
    "    print_verification_results(X)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18218514",
   "metadata": {},
   "source": [
    "## SeNet (VggFace2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faf8fcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SENet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv4): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = torch.load('D:/src_code/DNN_models/vggface_senet50_1.pt')\n",
    "cnn_model=cnn_model.to(device)\n",
    "cnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cb09e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avsavchenko\\AppData\\Local\\Temp\\ipykernel_18884\\2271549186.py:19: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = np.array(img.resize((224,224),Image.BILINEAR))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 42.60752272605896 seconds ---\n",
      "after loading: X shape: (3739, 2048)  X_norm shape: (3739, 2048)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X=extract_torch_features(cnn_model,files, standard_preprocessing=False)\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "X_norm=preprocessing.normalize(X,norm='l2')\n",
    "X_norm=X_norm[indices,:]\n",
    "print('after loading: X shape:',X.shape,' X_norm shape:',X_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84351683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [98.95004773 98.79096405 84.66433344 98.02736239 98.63188037 98.25007954\n",
      " 98.79096405 98.85459752 98.21826281 98.88641425]\n",
      "total acc= 97.21 4.19\n",
      "test time= [0.10276651 0.10775042 0.10376048 0.10375953 0.10376334 0.10475445\n",
      " 0.10176659 0.10177088 0.09973216 0.10176492]\n",
      "\n",
      "k-NN\n",
      "accuracies= [98.91823099 98.91823099 84.6325167  98.1228126  98.50461343 98.25007954\n",
      " 98.82278078 98.82278078 98.37734648 98.91823099]\n",
      "total acc= 97.23 4.21\n",
      "test time= [0.19647241 0.19351745 0.20142484 0.20349264 0.19643807 0.20541239\n",
      " 0.19843364 0.20046544 0.18350315 0.18451214]\n",
      "\n",
      "linear svm\n",
      "accuracies= [98.88641425 98.91823099 87.97327394 98.40916322 98.5682469  98.53643016\n",
      " 98.88641425 98.88641425 98.69551384 99.0136812 ]\n",
      "total acc= 97.68 3.24\n",
      "test time= [0.0748024  0.07675815 0.07280827 0.0828011  0.07882667 0.07879925\n",
      " 0.07081342 0.06882143 0.06784153 0.07679391]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Recognition:')\n",
    "test_faceID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40a694f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avsavchenko\\AppData\\Local\\Temp\\ipykernel_18884\\2271549186.py:19: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = np.array(img.resize((224,224),Image.BILINEAR))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 78.83677887916565 seconds ---\n",
      "Verification\n",
      "(7701, 2048)\n",
      "Accuracy: 0.99367+-0.00371\n",
      "Validation rate: 0.97833+-0.01276 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99964\n",
      "Equal Error Rate (EER): 0.006\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X=extract_torch_features(cnn_model,files4verification, standard_preprocessing=False)\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "print('Verification')\n",
    "\n",
    "print(X.shape)\n",
    "X=preprocessing.normalize(X,norm='l2')\n",
    "\n",
    "print_verification_results(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9234bf",
   "metadata": {},
   "source": [
    "# Insightface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3617192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python38\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "<insightface.model_zoo.arcface_onnx.ArcFaceONNX object at 0x0000015173DCB6D0>\n"
     ]
    }
   ],
   "source": [
    "import insightface\n",
    "#model_path='C:/Users/avsavchenko/.insightface/models/buffalo_l/w600k_r50.onnx'\n",
    "model_path='D:/src_code/DNN_models/age_gender/insightface/models/vgg2_r50_pfc.onnx'\n",
    "cnn_model = insightface.model_zoo.get_model(model_path,providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "cnn_model.prepare(ctx_id=0)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afc00a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_insightface_features(model,img_filepath):\n",
    "    img = cv2.imread(img_filepath)\n",
    "    img=img[:, :, ::-1] #bgr2rgb\n",
    "    embeddings = model.get_feat(img)\n",
    "    if embeddings is None:\n",
    "        print(img_filepath)\n",
    "    #print(embeddings.shape)\n",
    "    return embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f1a82dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 523.4497258663177 seconds ---\n",
      "after loading: X shape: (3739, 512)  X_norm shape: (3739, 512)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [99.17276487 98.98186446 98.63188037 99.0136812  98.95004773 99.14094814\n",
      " 99.1091314  98.91823099 99.14094814 99.26821508]\n",
      "total acc= 99.03 0.17\n",
      "test time= [0.09482813 0.09382486 0.09677649 0.09179568 0.09578252 0.09574437\n",
      " 0.0988121  0.09225655 0.0977385  0.09474683]\n",
      "\n",
      "k-NN\n",
      "accuracies= [99.30003182 99.33184855 98.88641425 99.33184855 99.20458161 99.14094814\n",
      " 99.1091314  99.17276487 99.30003182 99.49093223]\n",
      "total acc= 99.23 0.16\n",
      "test time= [0.10568261 0.11773086 0.10571599 0.10622478 0.107301   0.10942411\n",
      " 0.10870934 0.10779071 0.10775208 0.10622168]\n",
      "\n",
      "linear svm\n",
      "accuracies= [99.26821508 99.42729876 98.69551384 99.26821508 99.26821508 99.17276487\n",
      " 99.07731467 99.23639835 99.30003182 99.5545657 ]\n",
      "total acc= 99.23 0.22\n",
      "test time= [0.02994299 0.02800465 0.02993798 0.02991915 0.02692151 0.025038\n",
      " 0.03490615 0.03191566 0.02501273 0.02595019]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X=np.array([extract_insightface_features(cnn_model,os.path.join(DATASET_PATH,filepath)) for filepath in files])\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "X_norm=preprocessing.normalize(X,norm='l2')\n",
    "X_norm=X_norm[indices,:]\n",
    "print('after loading: X shape:',X.shape,' X_norm shape:',X_norm.shape)\n",
    "\n",
    "print('Recognition:')\n",
    "test_faceID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92850004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1063.6578254699707 seconds ---\n",
      "Verification\n",
      "(7701, 512)\n",
      "Accuracy: 0.99217+-0.00373\n",
      "Validation rate: 0.98567+-0.00978 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99929\n",
      "Equal Error Rate (EER): 0.006\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X=np.array([extract_insightface_features(cnn_model,os.path.join(DATASET_PATH,filepath)) for filepath in files4verification])\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "print('Verification')\n",
    "\n",
    "print(X.shape)\n",
    "X=preprocessing.normalize(X,norm='l2')\n",
    "\n",
    "print_verification_results(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d46674",
   "metadata": {},
   "source": [
    "# Tensorflow (FaceNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e727de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.compat.v1.keras.backend import set_session \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b8bcbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename, prefix=''):\n",
    "    with tf.io.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name=prefix)\n",
    "    return graph\n",
    "\n",
    "class TensorFlowInference:\n",
    "    def __init__(self,frozen_graph_filename,input_tensor,output_tensor,learning_phase_tensor=None, convert2BGR=True, imageNetUtilsMean=True,additional_input_value=0):\n",
    "        graph = load_graph(frozen_graph_filename,'')\n",
    "        print([n.name for n in graph.as_graph_def().node if 'input' in n.name])\n",
    "        \n",
    "        graph_op_list=list(graph.get_operations())\n",
    "        print([n.name for n in graph_op_list if 'keras_learning' in n.name])\n",
    "        \n",
    "        self.tf_sess=tf.compat.v1.Session(graph=graph)\n",
    "        \n",
    "        self.tf_input_image = graph.get_tensor_by_name(input_tensor)\n",
    "        print('tf_input_image=',self.tf_input_image)\n",
    "        self.tf_output_features = graph.get_tensor_by_name(output_tensor)\n",
    "        print('tf_output_features=',self.tf_output_features)\n",
    "        self.tf_learning_phase = graph.get_tensor_by_name(learning_phase_tensor) if learning_phase_tensor else None;\n",
    "        print('tf_learning_phase=',self.tf_learning_phase)\n",
    "        if self.tf_input_image.shape.dims is None:\n",
    "            w=h=160\n",
    "        else:\n",
    "            _,w,h,_=self.tf_input_image.shape\n",
    "        self.w,self.h=int(w),int(h)\n",
    "        print ('input w,h',self.w,self.h,' output shape:',self.tf_output_features.shape,'convert2BGR:',convert2BGR, 'imageNetUtilsMean:',imageNetUtilsMean)\n",
    "        #for n in graph.as_graph_def().node:\n",
    "        #    print(n.name, n.op)\n",
    "        #sys.exit(0)\n",
    "\n",
    "        self.convert2BGR=convert2BGR\n",
    "        self.imageNetUtilsMean=imageNetUtilsMean\n",
    "        self.additional_input_value=additional_input_value\n",
    "\n",
    "    def preprocess_image(self,img_filepath,crop_center):\n",
    "        if crop_center:\n",
    "            orig_w,orig_h=250,250\n",
    "            img = imread(img_filepath)#, mode='RGB')\n",
    "            #img = misc.imresize(img, (orig_w,orig_h), interp='bilinear')\n",
    "            img = np.array(Image.fromarray(img).resize((orig_w,orig_h),resample=Image.BILINEAR))\n",
    "            w1,h1=128,128\n",
    "            dw=(orig_w-w1)//2\n",
    "            dh=(orig_h-h1)//2\n",
    "            box = (dw, dh, orig_w-dw, orig_h-dh)\n",
    "            img=img[dh:-dh,dw:-dw]\n",
    "        else:\n",
    "            img = imread(img_filepath)#, mode='RGB')\n",
    "        \n",
    "        #x = misc.imresize(img, (self.w,self.h), interp='bilinear').astype(float)\n",
    "        x = np.array(Image.fromarray(img).resize((self.w,self.h),resample=Image.BILINEAR)).astype(float)\n",
    "        \n",
    "        if self.convert2BGR:\n",
    "            # 'RGB'->'BGR'\n",
    "            x = x[..., ::-1]\n",
    "            # Zero-center by mean pixel\n",
    "            if self.imageNetUtilsMean: #imagenet.utils caffe\n",
    "                x[..., 0] -= 103.939\n",
    "                x[..., 1] -= 116.779\n",
    "                x[..., 2] -= 123.68\n",
    "            else: #vggface-2\n",
    "                x[..., 0] -= 91.4953\n",
    "                x[..., 1] -= 103.8827\n",
    "                x[..., 2] -= 131.0912\n",
    "        else:\n",
    "            #x=(x-127.5)/128.0\n",
    "            x /= 127.5\n",
    "            x -= 1.\n",
    "            #x=x/128.0-1.0\n",
    "        return x\n",
    "        \n",
    "    def extract_features(self,img_filepath,crop_center=False):\n",
    "        x=self.preprocess_image(img_filepath,crop_center)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        feed_dict={self.tf_input_image: x}\n",
    "        if self.tf_learning_phase is not None:\n",
    "            feed_dict[self.tf_learning_phase]=self.additional_input_value\n",
    "        preds = self.tf_sess.run(self.tf_output_features, feed_dict=feed_dict).reshape(-1)\n",
    "        #preds = self.tf_sess.run(self.tf_output_features, feed_dict=feed_dict).mean(axis=(0,1,2)).reshape(-1)\n",
    "        return preds\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.tf_sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ef70b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input']\n",
      "[]\n",
      "tf_input_image= Tensor(\"input:0\", dtype=float32)\n",
      "tf_output_features= Tensor(\"embeddings:0\", shape=(None, 512), dtype=float32)\n",
      "tf_learning_phase= Tensor(\"phase_train:0\", dtype=bool)\n",
      "input w,h 160 160  output shape: (None, 512) convert2BGR: False imageNetUtilsMean: True\n"
     ]
    }
   ],
   "source": [
    "#cnn_model=TensorFlowInference('D:/src_code/HSE_FaceRec_tf/age_gender_identity/age_gender_tf2_new-01-0.14-0.92.pb',input_tensor='input_1:0',output_tensor='global_pooling/Mean:0',convert2BGR=True, imageNetUtilsMean=True)\n",
    "\n",
    "#FaceNet\n",
    "cnn_model=TensorFlowInference('D:/src_code/DNN_models/facenet_inceptionresnet/20180402-114759.pb',input_tensor='input:0',output_tensor='embeddings:0',learning_phase_tensor='phase_train:0',convert2BGR=False) #embeddings, InceptionResnetV1/Repeat_2/block8_5/Relu, InceptionResnetV1/Repeat_1/block17_10/Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02008535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avsavchenko\\AppData\\Local\\Temp\\ipykernel_18884\\1513849870.py:55: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  x = np.array(Image.fromarray(img).resize((self.w,self.h),resample=Image.BILINEAR)).astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 120.72068953514099 seconds ---\n",
      "after loading: X shape: (3739, 512)  X_norm shape: (3739, 512)\n",
      "Recognition:\n",
      "k-NN+PCA\n",
      "accuracies= [97.39102768 97.74101177 85.93700286 96.85014318 96.7865097  96.75469297\n",
      " 97.6773783  97.42284442 96.97741012 97.6773783 ]\n",
      "total acc= 96.12 3.41\n",
      "test time= [0.08680582 0.08481026 0.08580971 0.08681345 0.088763   0.08581114\n",
      " 0.09578013 0.08676887 0.08680677 0.08776951]\n",
      "\n",
      "k-NN\n",
      "accuracies= [97.39102768 97.74101177 85.93700286 96.85014318 96.7865097  96.75469297\n",
      " 97.6773783  97.42284442 96.97741012 97.6773783 ]\n",
      "total acc= 96.12 3.41\n",
      "test time= [0.09674358 0.10072756 0.10169077 0.10475898 0.09973192 0.10172582\n",
      " 0.10268569 0.10375977 0.09873319 0.10269022]\n",
      "\n",
      "linear svm\n",
      "accuracies= [96.59560929 96.91377665 88.32325803 95.73655743 96.53197582 95.89564111\n",
      " 97.35921094 96.85014318 96.46834235 96.75469297]\n",
      "total acc= 95.74 2.51\n",
      "test time= [0.02594924 0.02695608 0.02593422 0.02593088 0.02489567 0.02596712\n",
      " 0.02493906 0.02495575 0.02594829 0.02496123]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crop_center=False\n",
    "start_time = time.time()\n",
    "X=np.array([cnn_model.extract_features(os.path.join(DATASET_PATH,filepath),crop_center=crop_center) for filepath in files])    \n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "X_norm=preprocessing.normalize(X,norm='l2')\n",
    "X_norm=X_norm[indices,:]\n",
    "print('after loading: X shape:',X.shape,' X_norm shape:',X_norm.shape)\n",
    "\n",
    "print('Recognition:')\n",
    "test_faceID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "798a6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avsavchenko\\AppData\\Local\\Temp\\ipykernel_18884\\1513849870.py:55: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  x = np.array(Image.fromarray(img).resize((self.w,self.h),resample=Image.BILINEAR)).astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 235.93100428581238 seconds ---\n",
      "Verification\n",
      "(7701, 512)\n",
      "Accuracy: 0.99283+-0.00527\n",
      "Validation rate: 0.97667+-0.01430 @ FAR=0.00100\n",
      "Area Under Curve (AUC): 0.99939\n",
      "Equal Error Rate (EER): 0.008\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X=np.array([cnn_model.extract_features(os.path.join(DATASET_PATH,filepath),crop_center=crop_center) for filepath in files4verification])    \n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "print('Verification')\n",
    "\n",
    "print(X.shape)\n",
    "X=preprocessing.normalize(X,norm='l2')\n",
    "\n",
    "print_verification_results(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8692448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
